{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mine ou rocher.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNrl5gYgZzbBJJFRRPNr1iB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/so-nafy/Tp_deepLearning/blob/main/Mine_ou_rocher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnJ2stGajjl1"
      },
      "source": [
        "Débugger un algo de Deep Learning : 4 réseaux profonds avec différents paramètres (12, 24, 26, 31 neurones, ...) ? \r\n",
        "=> 1è pas vers le tuning pour Décrypter les signaux d'un sonar : mine Vs. Rock : \r\n",
        "[Mine_ou_Rocher_12_neurones.py] [Mine_ou_Rocher_24_neurones.py]\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kppgvAA1mzGs"
      },
      "source": [
        "import pandas as pd "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "rCFcmUJ8m1HW",
        "outputId": "f790aab8-49f2-4ad2-9c51-3b8e145bf6c3"
      },
      "source": [
        "#Télécharger des données depuis votre disque local\r\n",
        "from google.colab import files\r\n",
        "data_to_load = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-72cb367e-381b-4c05-a8d4-0498ad0438e5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-72cb367e-381b-4c05-a8d4-0498ad0438e5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving sonar.all-data.csv to sonar.all-data (1).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "CDTt_WainHGq",
        "outputId": "fba6b0a9-1223-4fb4-e199-c8ea47e20074"
      },
      "source": [
        "import io\r\n",
        "observations = pd.read_csv(io.BytesIO(data_to_load['sonar.all-data.csv']))\r\n",
        "observations"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0.0200</th>\n",
              "      <th>0.0371</th>\n",
              "      <th>0.0428</th>\n",
              "      <th>0.0207</th>\n",
              "      <th>0.0954</th>\n",
              "      <th>0.0986</th>\n",
              "      <th>0.1539</th>\n",
              "      <th>0.1601</th>\n",
              "      <th>0.3109</th>\n",
              "      <th>0.2111</th>\n",
              "      <th>0.1609</th>\n",
              "      <th>0.1582</th>\n",
              "      <th>0.2238</th>\n",
              "      <th>0.0645</th>\n",
              "      <th>0.0660</th>\n",
              "      <th>0.2273</th>\n",
              "      <th>0.3100</th>\n",
              "      <th>0.2999</th>\n",
              "      <th>0.5078</th>\n",
              "      <th>0.4797</th>\n",
              "      <th>0.5783</th>\n",
              "      <th>0.5071</th>\n",
              "      <th>0.4328</th>\n",
              "      <th>0.5550</th>\n",
              "      <th>0.6711</th>\n",
              "      <th>0.6415</th>\n",
              "      <th>0.7104</th>\n",
              "      <th>0.8080</th>\n",
              "      <th>0.6791</th>\n",
              "      <th>0.3857</th>\n",
              "      <th>0.1307</th>\n",
              "      <th>0.2604</th>\n",
              "      <th>0.5121</th>\n",
              "      <th>0.7547</th>\n",
              "      <th>0.8537</th>\n",
              "      <th>0.8507</th>\n",
              "      <th>0.6692</th>\n",
              "      <th>0.6097</th>\n",
              "      <th>0.4943</th>\n",
              "      <th>0.2744</th>\n",
              "      <th>0.0510</th>\n",
              "      <th>0.2834</th>\n",
              "      <th>0.2825</th>\n",
              "      <th>0.4256</th>\n",
              "      <th>0.2641</th>\n",
              "      <th>0.1386</th>\n",
              "      <th>0.1051</th>\n",
              "      <th>0.1343</th>\n",
              "      <th>0.0383</th>\n",
              "      <th>0.0324</th>\n",
              "      <th>0.0232</th>\n",
              "      <th>0.0027</th>\n",
              "      <th>0.0065</th>\n",
              "      <th>0.0159</th>\n",
              "      <th>0.0072</th>\n",
              "      <th>0.0167</th>\n",
              "      <th>0.0180</th>\n",
              "      <th>0.0084</th>\n",
              "      <th>0.0090</th>\n",
              "      <th>0.0032</th>\n",
              "      <th>R</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>0.0843</td>\n",
              "      <td>0.0689</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.2583</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.3481</td>\n",
              "      <td>0.3337</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>0.4918</td>\n",
              "      <td>0.6552</td>\n",
              "      <td>0.6919</td>\n",
              "      <td>0.7797</td>\n",
              "      <td>0.7464</td>\n",
              "      <td>0.9444</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8874</td>\n",
              "      <td>0.8024</td>\n",
              "      <td>0.7818</td>\n",
              "      <td>0.5212</td>\n",
              "      <td>0.4052</td>\n",
              "      <td>0.3957</td>\n",
              "      <td>0.3914</td>\n",
              "      <td>0.3250</td>\n",
              "      <td>0.3200</td>\n",
              "      <td>0.3271</td>\n",
              "      <td>0.2767</td>\n",
              "      <td>0.4423</td>\n",
              "      <td>0.2028</td>\n",
              "      <td>0.3788</td>\n",
              "      <td>0.2947</td>\n",
              "      <td>0.1984</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.1306</td>\n",
              "      <td>0.4182</td>\n",
              "      <td>0.3835</td>\n",
              "      <td>0.1057</td>\n",
              "      <td>0.1840</td>\n",
              "      <td>0.1970</td>\n",
              "      <td>0.1674</td>\n",
              "      <td>0.0583</td>\n",
              "      <td>0.1401</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.0621</td>\n",
              "      <td>0.0203</td>\n",
              "      <td>0.0530</td>\n",
              "      <td>0.0742</td>\n",
              "      <td>0.0409</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0125</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0.0191</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>0.1099</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0974</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>0.3771</td>\n",
              "      <td>0.5598</td>\n",
              "      <td>0.6194</td>\n",
              "      <td>0.6333</td>\n",
              "      <td>0.7060</td>\n",
              "      <td>0.5544</td>\n",
              "      <td>0.5320</td>\n",
              "      <td>0.6479</td>\n",
              "      <td>0.6931</td>\n",
              "      <td>0.6759</td>\n",
              "      <td>0.7551</td>\n",
              "      <td>0.8929</td>\n",
              "      <td>0.8619</td>\n",
              "      <td>0.7974</td>\n",
              "      <td>0.6737</td>\n",
              "      <td>0.4293</td>\n",
              "      <td>0.3648</td>\n",
              "      <td>0.5331</td>\n",
              "      <td>0.2413</td>\n",
              "      <td>0.5070</td>\n",
              "      <td>0.8533</td>\n",
              "      <td>0.6036</td>\n",
              "      <td>0.8514</td>\n",
              "      <td>0.8512</td>\n",
              "      <td>0.5045</td>\n",
              "      <td>0.1862</td>\n",
              "      <td>0.2709</td>\n",
              "      <td>0.4232</td>\n",
              "      <td>0.3043</td>\n",
              "      <td>0.6116</td>\n",
              "      <td>0.6756</td>\n",
              "      <td>0.5375</td>\n",
              "      <td>0.4719</td>\n",
              "      <td>0.4647</td>\n",
              "      <td>0.2587</td>\n",
              "      <td>0.2129</td>\n",
              "      <td>0.2222</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.0176</td>\n",
              "      <td>0.1348</td>\n",
              "      <td>0.0744</td>\n",
              "      <td>0.0130</td>\n",
              "      <td>0.0106</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0078</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1276</td>\n",
              "      <td>0.0598</td>\n",
              "      <td>0.1264</td>\n",
              "      <td>0.0881</td>\n",
              "      <td>0.1992</td>\n",
              "      <td>0.0184</td>\n",
              "      <td>0.2261</td>\n",
              "      <td>0.1729</td>\n",
              "      <td>0.2131</td>\n",
              "      <td>0.0693</td>\n",
              "      <td>0.2281</td>\n",
              "      <td>0.4060</td>\n",
              "      <td>0.3973</td>\n",
              "      <td>0.2741</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>0.5556</td>\n",
              "      <td>0.4846</td>\n",
              "      <td>0.3140</td>\n",
              "      <td>0.5334</td>\n",
              "      <td>0.5256</td>\n",
              "      <td>0.2520</td>\n",
              "      <td>0.2090</td>\n",
              "      <td>0.3559</td>\n",
              "      <td>0.6260</td>\n",
              "      <td>0.7340</td>\n",
              "      <td>0.6120</td>\n",
              "      <td>0.3497</td>\n",
              "      <td>0.3953</td>\n",
              "      <td>0.3012</td>\n",
              "      <td>0.5408</td>\n",
              "      <td>0.8814</td>\n",
              "      <td>0.9857</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>0.6121</td>\n",
              "      <td>0.5006</td>\n",
              "      <td>0.3210</td>\n",
              "      <td>0.3202</td>\n",
              "      <td>0.4295</td>\n",
              "      <td>0.3654</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>0.1576</td>\n",
              "      <td>0.0681</td>\n",
              "      <td>0.0294</td>\n",
              "      <td>0.0241</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0117</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0762</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.0394</td>\n",
              "      <td>0.0590</td>\n",
              "      <td>0.0649</td>\n",
              "      <td>0.1209</td>\n",
              "      <td>0.2467</td>\n",
              "      <td>0.3564</td>\n",
              "      <td>0.4459</td>\n",
              "      <td>0.4152</td>\n",
              "      <td>0.3952</td>\n",
              "      <td>0.4256</td>\n",
              "      <td>0.4135</td>\n",
              "      <td>0.4528</td>\n",
              "      <td>0.5326</td>\n",
              "      <td>0.7306</td>\n",
              "      <td>0.6193</td>\n",
              "      <td>0.2032</td>\n",
              "      <td>0.4636</td>\n",
              "      <td>0.4148</td>\n",
              "      <td>0.4292</td>\n",
              "      <td>0.5730</td>\n",
              "      <td>0.5399</td>\n",
              "      <td>0.3161</td>\n",
              "      <td>0.2285</td>\n",
              "      <td>0.6995</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.7262</td>\n",
              "      <td>0.4724</td>\n",
              "      <td>0.5103</td>\n",
              "      <td>0.5459</td>\n",
              "      <td>0.2881</td>\n",
              "      <td>0.0981</td>\n",
              "      <td>0.1951</td>\n",
              "      <td>0.4181</td>\n",
              "      <td>0.4604</td>\n",
              "      <td>0.3217</td>\n",
              "      <td>0.2828</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.1979</td>\n",
              "      <td>0.2444</td>\n",
              "      <td>0.1847</td>\n",
              "      <td>0.0841</td>\n",
              "      <td>0.0692</td>\n",
              "      <td>0.0528</td>\n",
              "      <td>0.0357</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.0046</td>\n",
              "      <td>0.0156</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0107</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0286</td>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0277</td>\n",
              "      <td>0.0174</td>\n",
              "      <td>0.0384</td>\n",
              "      <td>0.0990</td>\n",
              "      <td>0.1201</td>\n",
              "      <td>0.1833</td>\n",
              "      <td>0.2105</td>\n",
              "      <td>0.3039</td>\n",
              "      <td>0.2988</td>\n",
              "      <td>0.4250</td>\n",
              "      <td>0.6343</td>\n",
              "      <td>0.8198</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9988</td>\n",
              "      <td>0.9508</td>\n",
              "      <td>0.9025</td>\n",
              "      <td>0.7234</td>\n",
              "      <td>0.5122</td>\n",
              "      <td>0.2074</td>\n",
              "      <td>0.3985</td>\n",
              "      <td>0.5890</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>0.2043</td>\n",
              "      <td>0.5782</td>\n",
              "      <td>0.5389</td>\n",
              "      <td>0.3750</td>\n",
              "      <td>0.3411</td>\n",
              "      <td>0.5067</td>\n",
              "      <td>0.5580</td>\n",
              "      <td>0.4778</td>\n",
              "      <td>0.3299</td>\n",
              "      <td>0.2198</td>\n",
              "      <td>0.1407</td>\n",
              "      <td>0.2856</td>\n",
              "      <td>0.3807</td>\n",
              "      <td>0.4158</td>\n",
              "      <td>0.4054</td>\n",
              "      <td>0.3296</td>\n",
              "      <td>0.2707</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.0723</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1192</td>\n",
              "      <td>0.1089</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0494</td>\n",
              "      <td>0.0264</td>\n",
              "      <td>0.0081</td>\n",
              "      <td>0.0104</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0038</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0057</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>0.0062</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>0.0187</td>\n",
              "      <td>0.0346</td>\n",
              "      <td>0.0168</td>\n",
              "      <td>0.0177</td>\n",
              "      <td>0.0393</td>\n",
              "      <td>0.1630</td>\n",
              "      <td>0.2028</td>\n",
              "      <td>0.1694</td>\n",
              "      <td>0.2328</td>\n",
              "      <td>0.2684</td>\n",
              "      <td>0.3108</td>\n",
              "      <td>0.2933</td>\n",
              "      <td>0.2275</td>\n",
              "      <td>0.0994</td>\n",
              "      <td>0.1801</td>\n",
              "      <td>0.2200</td>\n",
              "      <td>0.2732</td>\n",
              "      <td>0.2862</td>\n",
              "      <td>0.2034</td>\n",
              "      <td>0.1740</td>\n",
              "      <td>0.4130</td>\n",
              "      <td>0.6879</td>\n",
              "      <td>0.8120</td>\n",
              "      <td>0.8453</td>\n",
              "      <td>0.8919</td>\n",
              "      <td>0.9300</td>\n",
              "      <td>0.9987</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8104</td>\n",
              "      <td>0.6199</td>\n",
              "      <td>0.6041</td>\n",
              "      <td>0.5547</td>\n",
              "      <td>0.4160</td>\n",
              "      <td>0.1472</td>\n",
              "      <td>0.0849</td>\n",
              "      <td>0.0608</td>\n",
              "      <td>0.0969</td>\n",
              "      <td>0.1411</td>\n",
              "      <td>0.1676</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.1201</td>\n",
              "      <td>0.1036</td>\n",
              "      <td>0.1977</td>\n",
              "      <td>0.1339</td>\n",
              "      <td>0.0902</td>\n",
              "      <td>0.1085</td>\n",
              "      <td>0.1521</td>\n",
              "      <td>0.1363</td>\n",
              "      <td>0.0858</td>\n",
              "      <td>0.0290</td>\n",
              "      <td>0.0203</td>\n",
              "      <td>0.0116</td>\n",
              "      <td>0.0098</td>\n",
              "      <td>0.0199</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0101</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0115</td>\n",
              "      <td>0.0193</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>0.0323</td>\n",
              "      <td>0.0101</td>\n",
              "      <td>0.0298</td>\n",
              "      <td>0.0564</td>\n",
              "      <td>0.0760</td>\n",
              "      <td>0.0958</td>\n",
              "      <td>0.0990</td>\n",
              "      <td>0.1018</td>\n",
              "      <td>0.1030</td>\n",
              "      <td>0.2154</td>\n",
              "      <td>0.3085</td>\n",
              "      <td>0.3425</td>\n",
              "      <td>0.2990</td>\n",
              "      <td>0.1402</td>\n",
              "      <td>0.1235</td>\n",
              "      <td>0.1534</td>\n",
              "      <td>0.1901</td>\n",
              "      <td>0.2429</td>\n",
              "      <td>0.2120</td>\n",
              "      <td>0.2395</td>\n",
              "      <td>0.3272</td>\n",
              "      <td>0.5949</td>\n",
              "      <td>0.8302</td>\n",
              "      <td>0.9045</td>\n",
              "      <td>0.9888</td>\n",
              "      <td>0.9912</td>\n",
              "      <td>0.9448</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9092</td>\n",
              "      <td>0.7412</td>\n",
              "      <td>0.7691</td>\n",
              "      <td>0.7117</td>\n",
              "      <td>0.5304</td>\n",
              "      <td>0.2131</td>\n",
              "      <td>0.0928</td>\n",
              "      <td>0.1297</td>\n",
              "      <td>0.1159</td>\n",
              "      <td>0.1226</td>\n",
              "      <td>0.1768</td>\n",
              "      <td>0.0345</td>\n",
              "      <td>0.1562</td>\n",
              "      <td>0.0824</td>\n",
              "      <td>0.1149</td>\n",
              "      <td>0.1694</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0080</td>\n",
              "      <td>0.0790</td>\n",
              "      <td>0.1255</td>\n",
              "      <td>0.0647</td>\n",
              "      <td>0.0179</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0093</td>\n",
              "      <td>0.0135</td>\n",
              "      <td>0.0063</td>\n",
              "      <td>0.0063</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0062</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>0.0522</td>\n",
              "      <td>0.0437</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0292</td>\n",
              "      <td>0.0351</td>\n",
              "      <td>0.1171</td>\n",
              "      <td>0.1257</td>\n",
              "      <td>0.1178</td>\n",
              "      <td>0.1258</td>\n",
              "      <td>0.2529</td>\n",
              "      <td>0.2716</td>\n",
              "      <td>0.2374</td>\n",
              "      <td>0.1878</td>\n",
              "      <td>0.0983</td>\n",
              "      <td>0.0683</td>\n",
              "      <td>0.1503</td>\n",
              "      <td>0.1723</td>\n",
              "      <td>0.2339</td>\n",
              "      <td>0.1962</td>\n",
              "      <td>0.1395</td>\n",
              "      <td>0.3164</td>\n",
              "      <td>0.5888</td>\n",
              "      <td>0.7631</td>\n",
              "      <td>0.8473</td>\n",
              "      <td>0.9424</td>\n",
              "      <td>0.9986</td>\n",
              "      <td>0.9699</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8630</td>\n",
              "      <td>0.6979</td>\n",
              "      <td>0.7717</td>\n",
              "      <td>0.7305</td>\n",
              "      <td>0.5197</td>\n",
              "      <td>0.1786</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1446</td>\n",
              "      <td>0.1066</td>\n",
              "      <td>0.1440</td>\n",
              "      <td>0.1929</td>\n",
              "      <td>0.0325</td>\n",
              "      <td>0.1490</td>\n",
              "      <td>0.0328</td>\n",
              "      <td>0.0537</td>\n",
              "      <td>0.1309</td>\n",
              "      <td>0.0910</td>\n",
              "      <td>0.0757</td>\n",
              "      <td>0.1059</td>\n",
              "      <td>0.1005</td>\n",
              "      <td>0.0535</td>\n",
              "      <td>0.0235</td>\n",
              "      <td>0.0155</td>\n",
              "      <td>0.0160</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>0.0062</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0138</td>\n",
              "      <td>0.0077</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>0.0303</td>\n",
              "      <td>0.0353</td>\n",
              "      <td>0.0490</td>\n",
              "      <td>0.0608</td>\n",
              "      <td>0.0167</td>\n",
              "      <td>0.1354</td>\n",
              "      <td>0.1465</td>\n",
              "      <td>0.1123</td>\n",
              "      <td>0.1945</td>\n",
              "      <td>0.2354</td>\n",
              "      <td>0.2898</td>\n",
              "      <td>0.2812</td>\n",
              "      <td>0.1578</td>\n",
              "      <td>0.0273</td>\n",
              "      <td>0.0673</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.2070</td>\n",
              "      <td>0.2645</td>\n",
              "      <td>0.2828</td>\n",
              "      <td>0.4293</td>\n",
              "      <td>0.5685</td>\n",
              "      <td>0.6990</td>\n",
              "      <td>0.7246</td>\n",
              "      <td>0.7622</td>\n",
              "      <td>0.9242</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9979</td>\n",
              "      <td>0.8297</td>\n",
              "      <td>0.7032</td>\n",
              "      <td>0.7141</td>\n",
              "      <td>0.6893</td>\n",
              "      <td>0.4961</td>\n",
              "      <td>0.2584</td>\n",
              "      <td>0.0969</td>\n",
              "      <td>0.0776</td>\n",
              "      <td>0.0364</td>\n",
              "      <td>0.1572</td>\n",
              "      <td>0.1823</td>\n",
              "      <td>0.1349</td>\n",
              "      <td>0.0849</td>\n",
              "      <td>0.0492</td>\n",
              "      <td>0.1367</td>\n",
              "      <td>0.1552</td>\n",
              "      <td>0.1548</td>\n",
              "      <td>0.1319</td>\n",
              "      <td>0.0985</td>\n",
              "      <td>0.1258</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0489</td>\n",
              "      <td>0.0241</td>\n",
              "      <td>0.0042</td>\n",
              "      <td>0.0086</td>\n",
              "      <td>0.0046</td>\n",
              "      <td>0.0126</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0079</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>0.0260</td>\n",
              "      <td>0.0363</td>\n",
              "      <td>0.0136</td>\n",
              "      <td>0.0272</td>\n",
              "      <td>0.0214</td>\n",
              "      <td>0.0338</td>\n",
              "      <td>0.0655</td>\n",
              "      <td>0.1400</td>\n",
              "      <td>0.1843</td>\n",
              "      <td>0.2354</td>\n",
              "      <td>0.2720</td>\n",
              "      <td>0.2442</td>\n",
              "      <td>0.1665</td>\n",
              "      <td>0.0336</td>\n",
              "      <td>0.1302</td>\n",
              "      <td>0.1708</td>\n",
              "      <td>0.2177</td>\n",
              "      <td>0.3175</td>\n",
              "      <td>0.3714</td>\n",
              "      <td>0.4552</td>\n",
              "      <td>0.5700</td>\n",
              "      <td>0.7397</td>\n",
              "      <td>0.8062</td>\n",
              "      <td>0.8837</td>\n",
              "      <td>0.9432</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9375</td>\n",
              "      <td>0.7603</td>\n",
              "      <td>0.7123</td>\n",
              "      <td>0.8358</td>\n",
              "      <td>0.7622</td>\n",
              "      <td>0.4567</td>\n",
              "      <td>0.1715</td>\n",
              "      <td>0.1549</td>\n",
              "      <td>0.1641</td>\n",
              "      <td>0.1869</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>0.1713</td>\n",
              "      <td>0.0959</td>\n",
              "      <td>0.0768</td>\n",
              "      <td>0.0847</td>\n",
              "      <td>0.2076</td>\n",
              "      <td>0.2505</td>\n",
              "      <td>0.1862</td>\n",
              "      <td>0.1439</td>\n",
              "      <td>0.1470</td>\n",
              "      <td>0.0991</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0154</td>\n",
              "      <td>0.0116</td>\n",
              "      <td>0.0181</td>\n",
              "      <td>0.0146</td>\n",
              "      <td>0.0129</td>\n",
              "      <td>0.0047</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0115</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>207 rows × 61 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0.0200  0.0371  0.0428  0.0207  0.0954  ...  0.0180  0.0084  0.0090  0.0032  R\n",
              "0    0.0453  0.0523  0.0843  0.0689  0.1183  ...  0.0140  0.0049  0.0052  0.0044  R\n",
              "1    0.0262  0.0582  0.1099  0.1083  0.0974  ...  0.0316  0.0164  0.0095  0.0078  R\n",
              "2    0.0100  0.0171  0.0623  0.0205  0.0205  ...  0.0050  0.0044  0.0040  0.0117  R\n",
              "3    0.0762  0.0666  0.0481  0.0394  0.0590  ...  0.0072  0.0048  0.0107  0.0094  R\n",
              "4    0.0286  0.0453  0.0277  0.0174  0.0384  ...  0.0057  0.0027  0.0051  0.0062  R\n",
              "..      ...     ...     ...     ...     ...  ...     ...     ...     ...     ... ..\n",
              "202  0.0187  0.0346  0.0168  0.0177  0.0393  ...  0.0065  0.0115  0.0193  0.0157  M\n",
              "203  0.0323  0.0101  0.0298  0.0564  0.0760  ...  0.0034  0.0032  0.0062  0.0067  M\n",
              "204  0.0522  0.0437  0.0180  0.0292  0.0351  ...  0.0140  0.0138  0.0077  0.0031  M\n",
              "205  0.0303  0.0353  0.0490  0.0608  0.0167  ...  0.0034  0.0079  0.0036  0.0048  M\n",
              "206  0.0260  0.0363  0.0136  0.0272  0.0214  ...  0.0040  0.0036  0.0061  0.0115  M\n",
              "\n",
              "[207 rows x 61 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muBAPxfVofUN",
        "outputId": "2cc7537d-4a1a-4d52-e06b-d3c80ede7886"
      },
      "source": [
        "print(\"Nbr colonnes: \",len(observations.columns))\r\n",
        "#On ne prend que les données issues du sonar pour l'apprentissage\r\n",
        "X = observations[observations.columns[0:60]].values\r\n",
        "#On ne prend que les libellé\r\n",
        "y = observations[observations.columns[60]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nbr colonnes:  61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl3azuYBofsV"
      },
      "source": [
        "#On encode : Les mines sont égales à 0 et les rochers égaux à 1\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "encoder = LabelEncoder()\r\n",
        "encoder.fit(y)\r\n",
        "y = encoder.transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEFnvUdHofv0",
        "outputId": "8afc1379-ee6f-467a-8476-8629b974652b"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2VtZSExofz6"
      },
      "source": [
        "#On ajoute un encodage pour créer des classes :\r\n",
        "# Si c'est une mine [1,0]\r\n",
        "# Si c'est un rocher [0,1]\r\n",
        "import numpy as np\r\n",
        "n_labels = len(y)\r\n",
        "n_unique_labels = len(np.unique(y))\r\n",
        "one_hot_encode = np.zeros((n_labels,n_unique_labels))\r\n",
        "one_hot_encode[np.arange(n_labels),y] = 1\r\n",
        "Y=one_hot_encode\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luHDynFWof31",
        "outputId": "4e1e367f-2c27-4a81-ddb7-58e8728a32ec"
      },
      "source": [
        "Y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ctVPVxrof8U",
        "outputId": "78ef3e07-6800-426c-fd07-4ec92afc2c62"
      },
      "source": [
        "#Verification en prenant les enregistrement 0 et 97\r\n",
        "print(\"Classe Rocher:\",int(Y[0][1]))\r\n",
        "print(\"Classe Mine :\",int(Y[97][1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classe Rocher: 1\n",
            "Classe Mine : 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZgAP4tgpLvS"
      },
      "source": [
        "# CREATION DES JEUX D'APPRENTISSAGE ET DE TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu6DNJVRpHcV"
      },
      "source": [
        "#On mélange\r\n",
        "from sklearn.utils import shuffle\r\n",
        "X, Y = shuffle(X, Y, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDL5khf2pHix",
        "outputId": "b6e38109-d84c-4041-ccc0-c23d326237f9"
      },
      "source": [
        "X, Y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.0368, 0.0279, 0.0103, ..., 0.0086, 0.011 , 0.0052],\n",
              "        [0.0047, 0.0059, 0.008 , ..., 0.0045, 0.0002, 0.0029],\n",
              "        [0.0411, 0.0277, 0.0604, ..., 0.005 , 0.0085, 0.0044],\n",
              "        ...,\n",
              "        [0.0139, 0.0222, 0.0089, ..., 0.0059, 0.0039, 0.0048],\n",
              "        [0.0707, 0.1252, 0.1447, ..., 0.0131, 0.0154, 0.0218],\n",
              "        [0.0123, 0.0022, 0.0196, ..., 0.0058, 0.0047, 0.0071]]),\n",
              " array([[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0vEjMpupHpC"
      },
      "source": [
        "#Creation des jeux d'apprentissage\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.07, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh37dtU_ph-B"
      },
      "source": [
        "# PARAMETRAGE DU RESEAU DE  NEURONES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVAknbqopbPw"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\r\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7Qh6tthpbTH"
      },
      "source": [
        "#Variable TensorFLow correspondant aux 60 valeurs des neurones d'entrée\r\n",
        "tf_neurones_entrees_X = tf.placeholder(tf.float32,[None, 60])\r\n",
        "\r\n",
        "#Variable TensorFlow correspondant au 2 neurones de sortie\r\n",
        "tf_valeurs_reelles_Y = tf.placeholder(tf.float32,[None, 2])\r\n",
        "\r\n",
        "\r\n",
        "poids = {\r\n",
        "    # 60 neurones d'entrées vers 24 Neurones de la couche cachée\r\n",
        "    'couche_entree_vers_cachee': tf.Variable(tf.random_uniform([60, 24], minval=-0.3, maxval=0.3), tf.float32),\r\n",
        "\r\n",
        "    # 24 neurones de la couche cachée vers 2 de la couche de sortie\r\n",
        "    'couche_cachee_vers_sortie': tf.Variable(tf.random_uniform([24, 2], minval=-0.3, maxval=0.3), tf.float32),\r\n",
        "}\r\n",
        "\r\n",
        "poids_biais = {\r\n",
        "     #1 biais de la couche d'entrée vers les 24 neurones de la couche cachée\r\n",
        "    'poids_biais_couche_entree_vers_cachee': tf.Variable(tf.zeros([24]), tf.float32),\r\n",
        "\r\n",
        "    #1 biais de la couche cachée vers les 2 neurones de la couche de sortie\r\n",
        "    'poids_biais_couche_cachee_vers_sortie': tf.Variable(tf.zeros([2]), tf.float32),\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6V-djCsrVBE"
      },
      "source": [
        "FONCTION DE  CREATION DU RESEAU DE NEURONES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpaWS26gogAA"
      },
      "source": [
        "def reseau_neurones_multicouches(observations_en_entrees, poids, poids_biais):\r\n",
        "\r\n",
        "    #Calcul de l'activation de la première couche\r\n",
        "    premiere_activation = tf.sigmoid(tf.matmul(tf_neurones_entrees_X, poids['couche_entree_vers_cachee']) + poids_biais['poids_biais_couche_entree_vers_cachee'])\r\n",
        "\r\n",
        "    #Calcul de l'activation de la seconde couche\r\n",
        "    activation_couche_cachee = tf.sigmoid(tf.matmul(premiere_activation, poids['couche_cachee_vers_sortie']) + poids_biais['poids_biais_couche_cachee_vers_sortie'])\r\n",
        "\r\n",
        "    return activation_couche_cachee"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS7Lp0Un-FFQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNvxOqoRrmAF"
      },
      "source": [
        "CREATION DU RESEAU DE NEURONES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erBy4doO-Yx7"
      },
      "source": [
        "reseau = reseau_neurones_multicouches(tf_neurones_entrees_X, poids, poids_biais)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pivet5pr_65B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba107c2b-347b-47e3-ce4a-b425b8e6a628"
      },
      "source": [
        "reseau"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Sigmoid_3:0' shape=(?, 2) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OF99DVirtWj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxRiLDrIrwa1"
      },
      "source": [
        " ERREUR ET OPTIMISATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_e1JPX9rtj1"
      },
      "source": [
        "#Fonction d'erreur de moyenne quadratique MSE\r\n",
        "fonction_erreur = tf.reduce_sum(tf.pow(tf_valeurs_reelles_Y-reseau,2))\r\n",
        "\r\n",
        "#Descente de gradient avec un taux d'apprentissage fixé à 0.1\r\n",
        "optimiseur = tf.train.GradientDescentOptimizer(learning_rate=taux_apprentissage).minimize(fonction_erreur)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnIaGJkKsIV0"
      },
      "source": [
        "APPRENTISSAGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdqyVE48rt3X"
      },
      "source": [
        "#Initialisation des variable\r\n",
        "init = tf.global_variables_initializer()\r\n",
        "\r\n",
        "#Demarrage d'une session d'apprentissage\r\n",
        "session = tf.Session()\r\n",
        "session.run(init)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q8-2v4pCDna"
      },
      "source": [
        "#Pour la réalisation du graphique pour la MSE\r\n",
        "Graphique_MSE=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgG9yN3VsJx0",
        "outputId": "d58039b2-a036-49d0-b435-8989f9323bd2"
      },
      "source": [
        "#Pour chaque epoch\r\n",
        "for i in range(epochs):\r\n",
        "\r\n",
        "   #Realisation de l'apprentissage avec mise à jour des poids\r\n",
        "   session.run(optimiseur, feed_dict = {tf_neurones_entrees_X: train_x, tf_valeurs_reelles_Y:train_y})\r\n",
        "\r\n",
        "   #Calculer l'erreur\r\n",
        "   MSE = session.run(fonction_erreur, feed_dict = {tf_neurones_entrees_X: train_x, tf_valeurs_reelles_Y:train_y})\r\n",
        "\r\n",
        "   #Affichage des informations\r\n",
        "   Graphique_MSE.append(MSE)\r\n",
        "   print(\"EPOCH (\" + str(i) + \"/\" + str(epochs) + \") -  MSE: \"+ str(MSE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH (0/300) -  MSE: 52.54873\n",
            "EPOCH (1/300) -  MSE: 51.43132\n",
            "EPOCH (2/300) -  MSE: 52.460655\n",
            "EPOCH (3/300) -  MSE: 51.345222\n",
            "EPOCH (4/300) -  MSE: 52.37359\n",
            "EPOCH (5/300) -  MSE: 51.259995\n",
            "EPOCH (6/300) -  MSE: 52.28752\n",
            "EPOCH (7/300) -  MSE: 51.175617\n",
            "EPOCH (8/300) -  MSE: 52.202423\n",
            "EPOCH (9/300) -  MSE: 51.09207\n",
            "EPOCH (10/300) -  MSE: 52.11827\n",
            "EPOCH (11/300) -  MSE: 51.00932\n",
            "EPOCH (12/300) -  MSE: 52.03505\n",
            "EPOCH (13/300) -  MSE: 50.927353\n",
            "EPOCH (14/300) -  MSE: 51.952724\n",
            "EPOCH (15/300) -  MSE: 50.846153\n",
            "EPOCH (16/300) -  MSE: 51.871292\n",
            "EPOCH (17/300) -  MSE: 50.76569\n",
            "EPOCH (18/300) -  MSE: 51.790726\n",
            "EPOCH (19/300) -  MSE: 50.685955\n",
            "EPOCH (20/300) -  MSE: 51.711002\n",
            "EPOCH (21/300) -  MSE: 50.60692\n",
            "EPOCH (22/300) -  MSE: 51.632103\n",
            "EPOCH (23/300) -  MSE: 50.528564\n",
            "EPOCH (24/300) -  MSE: 51.554005\n",
            "EPOCH (25/300) -  MSE: 50.450867\n",
            "EPOCH (26/300) -  MSE: 51.476696\n",
            "EPOCH (27/300) -  MSE: 50.37382\n",
            "EPOCH (28/300) -  MSE: 51.400154\n",
            "EPOCH (29/300) -  MSE: 50.297398\n",
            "EPOCH (30/300) -  MSE: 51.324356\n",
            "EPOCH (31/300) -  MSE: 50.22158\n",
            "EPOCH (32/300) -  MSE: 51.249283\n",
            "EPOCH (33/300) -  MSE: 50.14634\n",
            "EPOCH (34/300) -  MSE: 51.17492\n",
            "EPOCH (35/300) -  MSE: 50.07168\n",
            "EPOCH (36/300) -  MSE: 51.10125\n",
            "EPOCH (37/300) -  MSE: 49.997566\n",
            "EPOCH (38/300) -  MSE: 51.028236\n",
            "EPOCH (39/300) -  MSE: 49.92398\n",
            "EPOCH (40/300) -  MSE: 50.955883\n",
            "EPOCH (41/300) -  MSE: 49.850918\n",
            "EPOCH (42/300) -  MSE: 50.88417\n",
            "EPOCH (43/300) -  MSE: 49.778347\n",
            "EPOCH (44/300) -  MSE: 50.813065\n",
            "EPOCH (45/300) -  MSE: 49.706264\n",
            "EPOCH (46/300) -  MSE: 50.74256\n",
            "EPOCH (47/300) -  MSE: 49.634644\n",
            "EPOCH (48/300) -  MSE: 50.672646\n",
            "EPOCH (49/300) -  MSE: 49.563473\n",
            "EPOCH (50/300) -  MSE: 50.603287\n",
            "EPOCH (51/300) -  MSE: 49.492733\n",
            "EPOCH (52/300) -  MSE: 50.534473\n",
            "EPOCH (53/300) -  MSE: 49.422413\n",
            "EPOCH (54/300) -  MSE: 50.4662\n",
            "EPOCH (55/300) -  MSE: 49.3525\n",
            "EPOCH (56/300) -  MSE: 50.39843\n",
            "EPOCH (57/300) -  MSE: 49.282967\n",
            "EPOCH (58/300) -  MSE: 50.33117\n",
            "EPOCH (59/300) -  MSE: 49.213806\n",
            "EPOCH (60/300) -  MSE: 50.26439\n",
            "EPOCH (61/300) -  MSE: 49.145008\n",
            "EPOCH (62/300) -  MSE: 50.198074\n",
            "EPOCH (63/300) -  MSE: 49.076553\n",
            "EPOCH (64/300) -  MSE: 50.132217\n",
            "EPOCH (65/300) -  MSE: 49.00844\n",
            "EPOCH (66/300) -  MSE: 50.066803\n",
            "EPOCH (67/300) -  MSE: 48.940636\n",
            "EPOCH (68/300) -  MSE: 50.001812\n",
            "EPOCH (69/300) -  MSE: 48.873146\n",
            "EPOCH (70/300) -  MSE: 49.93723\n",
            "EPOCH (71/300) -  MSE: 48.80594\n",
            "EPOCH (72/300) -  MSE: 49.873055\n",
            "EPOCH (73/300) -  MSE: 48.739017\n",
            "EPOCH (74/300) -  MSE: 49.809258\n",
            "EPOCH (75/300) -  MSE: 48.67237\n",
            "EPOCH (76/300) -  MSE: 49.745834\n",
            "EPOCH (77/300) -  MSE: 48.60598\n",
            "EPOCH (78/300) -  MSE: 49.682777\n",
            "EPOCH (79/300) -  MSE: 48.53984\n",
            "EPOCH (80/300) -  MSE: 49.620068\n",
            "EPOCH (81/300) -  MSE: 48.47393\n",
            "EPOCH (82/300) -  MSE: 49.55769\n",
            "EPOCH (83/300) -  MSE: 48.40825\n",
            "EPOCH (84/300) -  MSE: 49.49564\n",
            "EPOCH (85/300) -  MSE: 48.34279\n",
            "EPOCH (86/300) -  MSE: 49.433907\n",
            "EPOCH (87/300) -  MSE: 48.27753\n",
            "EPOCH (88/300) -  MSE: 49.37247\n",
            "EPOCH (89/300) -  MSE: 48.21247\n",
            "EPOCH (90/300) -  MSE: 49.311337\n",
            "EPOCH (91/300) -  MSE: 48.147602\n",
            "EPOCH (92/300) -  MSE: 49.250484\n",
            "EPOCH (93/300) -  MSE: 48.08291\n",
            "EPOCH (94/300) -  MSE: 49.1899\n",
            "EPOCH (95/300) -  MSE: 48.018387\n",
            "EPOCH (96/300) -  MSE: 49.129585\n",
            "EPOCH (97/300) -  MSE: 47.954033\n",
            "EPOCH (98/300) -  MSE: 49.069527\n",
            "EPOCH (99/300) -  MSE: 47.889835\n",
            "EPOCH (100/300) -  MSE: 49.009716\n",
            "EPOCH (101/300) -  MSE: 47.825783\n",
            "EPOCH (102/300) -  MSE: 48.950146\n",
            "EPOCH (103/300) -  MSE: 47.76187\n",
            "EPOCH (104/300) -  MSE: 48.890797\n",
            "EPOCH (105/300) -  MSE: 47.698097\n",
            "EPOCH (106/300) -  MSE: 48.831673\n",
            "EPOCH (107/300) -  MSE: 47.63445\n",
            "EPOCH (108/300) -  MSE: 48.772774\n",
            "EPOCH (109/300) -  MSE: 47.570923\n",
            "EPOCH (110/300) -  MSE: 48.71408\n",
            "EPOCH (111/300) -  MSE: 47.507507\n",
            "EPOCH (112/300) -  MSE: 48.655582\n",
            "EPOCH (113/300) -  MSE: 47.444202\n",
            "EPOCH (114/300) -  MSE: 48.59728\n",
            "EPOCH (115/300) -  MSE: 47.381\n",
            "EPOCH (116/300) -  MSE: 48.539158\n",
            "EPOCH (117/300) -  MSE: 47.3179\n",
            "EPOCH (118/300) -  MSE: 48.481216\n",
            "EPOCH (119/300) -  MSE: 47.254883\n",
            "EPOCH (120/300) -  MSE: 48.423447\n",
            "EPOCH (121/300) -  MSE: 47.19196\n",
            "EPOCH (122/300) -  MSE: 48.365852\n",
            "EPOCH (123/300) -  MSE: 47.12912\n",
            "EPOCH (124/300) -  MSE: 48.30842\n",
            "EPOCH (125/300) -  MSE: 47.06636\n",
            "EPOCH (126/300) -  MSE: 48.251137\n",
            "EPOCH (127/300) -  MSE: 47.00367\n",
            "EPOCH (128/300) -  MSE: 48.194008\n",
            "EPOCH (129/300) -  MSE: 46.94105\n",
            "EPOCH (130/300) -  MSE: 48.137016\n",
            "EPOCH (131/300) -  MSE: 46.878498\n",
            "EPOCH (132/300) -  MSE: 48.08017\n",
            "EPOCH (133/300) -  MSE: 46.81601\n",
            "EPOCH (134/300) -  MSE: 48.023453\n",
            "EPOCH (135/300) -  MSE: 46.753567\n",
            "EPOCH (136/300) -  MSE: 47.966866\n",
            "EPOCH (137/300) -  MSE: 46.691185\n",
            "EPOCH (138/300) -  MSE: 47.9104\n",
            "EPOCH (139/300) -  MSE: 46.628853\n",
            "EPOCH (140/300) -  MSE: 47.85405\n",
            "EPOCH (141/300) -  MSE: 46.566574\n",
            "EPOCH (142/300) -  MSE: 47.797813\n",
            "EPOCH (143/300) -  MSE: 46.50433\n",
            "EPOCH (144/300) -  MSE: 47.74169\n",
            "EPOCH (145/300) -  MSE: 46.442123\n",
            "EPOCH (146/300) -  MSE: 47.68566\n",
            "EPOCH (147/300) -  MSE: 46.37996\n",
            "EPOCH (148/300) -  MSE: 47.629734\n",
            "EPOCH (149/300) -  MSE: 46.317833\n",
            "EPOCH (150/300) -  MSE: 47.573902\n",
            "EPOCH (151/300) -  MSE: 46.255733\n",
            "EPOCH (152/300) -  MSE: 47.518158\n",
            "EPOCH (153/300) -  MSE: 46.193665\n",
            "EPOCH (154/300) -  MSE: 47.462498\n",
            "EPOCH (155/300) -  MSE: 46.131622\n",
            "EPOCH (156/300) -  MSE: 47.40692\n",
            "EPOCH (157/300) -  MSE: 46.069603\n",
            "EPOCH (158/300) -  MSE: 47.35141\n",
            "EPOCH (159/300) -  MSE: 46.0076\n",
            "EPOCH (160/300) -  MSE: 47.295982\n",
            "EPOCH (161/300) -  MSE: 45.945625\n",
            "EPOCH (162/300) -  MSE: 47.24062\n",
            "EPOCH (163/300) -  MSE: 45.88366\n",
            "EPOCH (164/300) -  MSE: 47.18531\n",
            "EPOCH (165/300) -  MSE: 45.821716\n",
            "EPOCH (166/300) -  MSE: 47.13007\n",
            "EPOCH (167/300) -  MSE: 45.759785\n",
            "EPOCH (168/300) -  MSE: 47.074875\n",
            "EPOCH (169/300) -  MSE: 45.697857\n",
            "EPOCH (170/300) -  MSE: 47.01973\n",
            "EPOCH (171/300) -  MSE: 45.635944\n",
            "EPOCH (172/300) -  MSE: 46.964638\n",
            "EPOCH (173/300) -  MSE: 45.574036\n",
            "EPOCH (174/300) -  MSE: 46.90958\n",
            "EPOCH (175/300) -  MSE: 45.51213\n",
            "EPOCH (176/300) -  MSE: 46.854553\n",
            "EPOCH (177/300) -  MSE: 45.450222\n",
            "EPOCH (178/300) -  MSE: 46.79956\n",
            "EPOCH (179/300) -  MSE: 45.38832\n",
            "EPOCH (180/300) -  MSE: 46.7446\n",
            "EPOCH (181/300) -  MSE: 45.326424\n",
            "EPOCH (182/300) -  MSE: 46.689663\n",
            "EPOCH (183/300) -  MSE: 45.26452\n",
            "EPOCH (184/300) -  MSE: 46.63475\n",
            "EPOCH (185/300) -  MSE: 45.202606\n",
            "EPOCH (186/300) -  MSE: 46.579845\n",
            "EPOCH (187/300) -  MSE: 45.140694\n",
            "EPOCH (188/300) -  MSE: 46.52495\n",
            "EPOCH (189/300) -  MSE: 45.078766\n",
            "EPOCH (190/300) -  MSE: 46.470062\n",
            "EPOCH (191/300) -  MSE: 45.016838\n",
            "EPOCH (192/300) -  MSE: 46.41518\n",
            "EPOCH (193/300) -  MSE: 44.954903\n",
            "EPOCH (194/300) -  MSE: 46.3603\n",
            "EPOCH (195/300) -  MSE: 44.89295\n",
            "EPOCH (196/300) -  MSE: 46.305412\n",
            "EPOCH (197/300) -  MSE: 44.830986\n",
            "EPOCH (198/300) -  MSE: 46.25051\n",
            "EPOCH (199/300) -  MSE: 44.769005\n",
            "EPOCH (200/300) -  MSE: 46.195595\n",
            "EPOCH (201/300) -  MSE: 44.70701\n",
            "EPOCH (202/300) -  MSE: 46.140663\n",
            "EPOCH (203/300) -  MSE: 44.644997\n",
            "EPOCH (204/300) -  MSE: 46.085712\n",
            "EPOCH (205/300) -  MSE: 44.58297\n",
            "EPOCH (206/300) -  MSE: 46.030727\n",
            "EPOCH (207/300) -  MSE: 44.52092\n",
            "EPOCH (208/300) -  MSE: 45.975716\n",
            "EPOCH (209/300) -  MSE: 44.458847\n",
            "EPOCH (210/300) -  MSE: 45.92067\n",
            "EPOCH (211/300) -  MSE: 44.39676\n",
            "EPOCH (212/300) -  MSE: 45.865585\n",
            "EPOCH (213/300) -  MSE: 44.33464\n",
            "EPOCH (214/300) -  MSE: 45.810463\n",
            "EPOCH (215/300) -  MSE: 44.27251\n",
            "EPOCH (216/300) -  MSE: 45.755287\n",
            "EPOCH (217/300) -  MSE: 44.210346\n",
            "EPOCH (218/300) -  MSE: 45.70006\n",
            "EPOCH (219/300) -  MSE: 44.148155\n",
            "EPOCH (220/300) -  MSE: 45.644783\n",
            "EPOCH (221/300) -  MSE: 44.085945\n",
            "EPOCH (222/300) -  MSE: 45.589447\n",
            "EPOCH (223/300) -  MSE: 44.023705\n",
            "EPOCH (224/300) -  MSE: 45.53405\n",
            "EPOCH (225/300) -  MSE: 43.961433\n",
            "EPOCH (226/300) -  MSE: 45.47858\n",
            "EPOCH (227/300) -  MSE: 43.89914\n",
            "EPOCH (228/300) -  MSE: 45.423046\n",
            "EPOCH (229/300) -  MSE: 43.836807\n",
            "EPOCH (230/300) -  MSE: 45.36744\n",
            "EPOCH (231/300) -  MSE: 43.77445\n",
            "EPOCH (232/300) -  MSE: 45.31175\n",
            "EPOCH (233/300) -  MSE: 43.712055\n",
            "EPOCH (234/300) -  MSE: 45.25598\n",
            "EPOCH (235/300) -  MSE: 43.64963\n",
            "EPOCH (236/300) -  MSE: 45.200134\n",
            "EPOCH (237/300) -  MSE: 43.58718\n",
            "EPOCH (238/300) -  MSE: 45.144196\n",
            "EPOCH (239/300) -  MSE: 43.524696\n",
            "EPOCH (240/300) -  MSE: 45.088165\n",
            "EPOCH (241/300) -  MSE: 43.46217\n",
            "EPOCH (242/300) -  MSE: 45.032043\n",
            "EPOCH (243/300) -  MSE: 43.399616\n",
            "EPOCH (244/300) -  MSE: 44.975822\n",
            "EPOCH (245/300) -  MSE: 43.33703\n",
            "EPOCH (246/300) -  MSE: 44.9195\n",
            "EPOCH (247/300) -  MSE: 43.274406\n",
            "EPOCH (248/300) -  MSE: 44.863075\n",
            "EPOCH (249/300) -  MSE: 43.211746\n",
            "EPOCH (250/300) -  MSE: 44.80655\n",
            "EPOCH (251/300) -  MSE: 43.149055\n",
            "EPOCH (252/300) -  MSE: 44.749897\n",
            "EPOCH (253/300) -  MSE: 43.086327\n",
            "EPOCH (254/300) -  MSE: 44.693142\n",
            "EPOCH (255/300) -  MSE: 43.02356\n",
            "EPOCH (256/300) -  MSE: 44.63627\n",
            "EPOCH (257/300) -  MSE: 42.960762\n",
            "EPOCH (258/300) -  MSE: 44.57928\n",
            "EPOCH (259/300) -  MSE: 42.89793\n",
            "EPOCH (260/300) -  MSE: 44.52218\n",
            "EPOCH (261/300) -  MSE: 42.835056\n",
            "EPOCH (262/300) -  MSE: 44.46494\n",
            "EPOCH (263/300) -  MSE: 42.772156\n",
            "EPOCH (264/300) -  MSE: 44.407585\n",
            "EPOCH (265/300) -  MSE: 42.709213\n",
            "EPOCH (266/300) -  MSE: 44.350098\n",
            "EPOCH (267/300) -  MSE: 42.646236\n",
            "EPOCH (268/300) -  MSE: 44.292484\n",
            "EPOCH (269/300) -  MSE: 42.583225\n",
            "EPOCH (270/300) -  MSE: 44.234734\n",
            "EPOCH (271/300) -  MSE: 42.520184\n",
            "EPOCH (272/300) -  MSE: 44.176857\n",
            "EPOCH (273/300) -  MSE: 42.457108\n",
            "EPOCH (274/300) -  MSE: 44.118843\n",
            "EPOCH (275/300) -  MSE: 42.393997\n",
            "EPOCH (276/300) -  MSE: 44.06069\n",
            "EPOCH (277/300) -  MSE: 42.330853\n",
            "EPOCH (278/300) -  MSE: 44.002396\n",
            "EPOCH (279/300) -  MSE: 42.267673\n",
            "EPOCH (280/300) -  MSE: 43.943962\n",
            "EPOCH (281/300) -  MSE: 42.20446\n",
            "EPOCH (282/300) -  MSE: 43.885387\n",
            "EPOCH (283/300) -  MSE: 42.14122\n",
            "EPOCH (284/300) -  MSE: 43.826668\n",
            "EPOCH (285/300) -  MSE: 42.077946\n",
            "EPOCH (286/300) -  MSE: 43.767807\n",
            "EPOCH (287/300) -  MSE: 42.01464\n",
            "EPOCH (288/300) -  MSE: 43.7088\n",
            "EPOCH (289/300) -  MSE: 41.95131\n",
            "EPOCH (290/300) -  MSE: 43.64965\n",
            "EPOCH (291/300) -  MSE: 41.887955\n",
            "EPOCH (292/300) -  MSE: 43.590347\n",
            "EPOCH (293/300) -  MSE: 41.82457\n",
            "EPOCH (294/300) -  MSE: 43.530907\n",
            "EPOCH (295/300) -  MSE: 41.76116\n",
            "EPOCH (296/300) -  MSE: 43.47131\n",
            "EPOCH (297/300) -  MSE: 41.69772\n",
            "EPOCH (298/300) -  MSE: 43.41157\n",
            "EPOCH (299/300) -  MSE: 41.634262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "iIJHi1GcsJ4p",
        "outputId": "98f3888e-a332-4319-c1d9-80b5381e5508"
      },
      "source": [
        "#Affichage graphique\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "plt.plot(Graphique_MSE)\r\n",
        "plt.ylabel('MSE')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrS3Yqw4sxD0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBsPlIgEs02k"
      },
      "source": [
        "VERIFICATION DE L'APPRENTISSAGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfMPwnhHsxSM"
      },
      "source": [
        "#Les probabilités de chaque classe 'Mine' ou 'rocher' issues de l'apprentissage sont stockée dans le modèle.\r\n",
        "#A l'aide de tf.argmax, on récupére les indexs des probabilités les plus elevées pour chaque observations\r\n",
        "#Ex: Si pour une observation nous avons [0.56, 0.89] renverra 1 car la valeur la plus élevée se trouve à l'index 1\r\n",
        "#Ex : Si pour une observation nous avons [0.90, 0.34 ]  renverra 0 car la valeur la plus élevée se trouve à l'index 0\r\n",
        "classifications = tf.argmax(reseau, 1)\r\n",
        "\r\n",
        "#Dans le tableau des valeurs réelles :\r\n",
        "#Les mines sont encodées comme suit [1,0] l'index ayant la plus grande valeur est 0\r\n",
        "#Les rochers ont pour valeur [0,1] sl'index ayant la plus grande valeur est 1\r\n",
        "\r\n",
        "#Si la classification est de [0.90, 0.34 ] l'index ayant la plus grande valeur est 0\r\n",
        "#Si c'est une mine [1,0] l'index ayant la plus grande valeur est 0\r\n",
        "#Si les deux index sont identiques alors on peut affirmer que c'est une bonne classification\r\n",
        "formule_calcul_bonnes_classifications = tf.equal(classifications, tf.argmax(tf_valeurs_reelles_Y,1))\r\n",
        "\r\n",
        "\r\n",
        "#La précision se calcul en faisant la moyenne (tf.mean)\r\n",
        "# des bonnes classifications (aprés les avoir converties en décimale tf.cast, tf.float32)\r\n",
        "formule_precision = tf.reduce_mean(tf.cast(formule_calcul_bonnes_classifications, tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7E16_NttRUB"
      },
      "source": [
        "PRECISION SUR LES DONNEES DE TESTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ezi9Tn_sxZF",
        "outputId": "2ce6c0bf-f252-4552-ca15-4ad85d8e13e4"
      },
      "source": [
        "nb_classifications = 0;\r\n",
        "nb_bonnes_classifications = 0\r\n",
        "\r\n",
        "#On parcours l'ensemble des données de test (text_x)\r\n",
        "for i in range(0,test_x.shape[0]):\r\n",
        "\r\n",
        "    #On récupere les informations\r\n",
        "    donneesSonar = test_x[i].reshape(1,60)\r\n",
        "    classificationAttendue = test_y[i].reshape(1,2)\r\n",
        "\r\n",
        "    # On réalise la classification\r\n",
        "    prediction_run = session.run(classifications, feed_dict={tf_neurones_entrees_X:donneesSonar})\r\n",
        "\r\n",
        "    #On calcule la précision de la classification à l'aide de la formule établie auparavant\r\n",
        "    accuracy_run = session.run(formule_precision, feed_dict={tf_neurones_entrees_X:donneesSonar, tf_valeurs_reelles_Y:classificationAttendue})\r\n",
        "\r\n",
        "\r\n",
        "    #On affiche pour observation la classe originale et la classification réalisée\r\n",
        "    print(i,\"Classe attendue: \", int(session.run(tf_valeurs_reelles_Y[i][1],feed_dict={tf_valeurs_reelles_Y:test_y})), \" Classification: \", prediction_run[0] )\r\n",
        "\r\n",
        "    nb_classifications = nb_classifications+1\r\n",
        "    if(accuracy_run*100 ==100):\r\n",
        "        nb_bonnes_classifications = nb_bonnes_classifications+1\r\n",
        "\r\n",
        "\r\n",
        "print(\"-------------\")\r\n",
        "print(\"Précision sur les donnees de tests = \"+str((nb_bonnes_classifications/nb_classifications)*100)+\"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Classe attendue:  1  Classification:  1\n",
            "1 Classe attendue:  0  Classification:  0\n",
            "2 Classe attendue:  1  Classification:  1\n",
            "3 Classe attendue:  1  Classification:  1\n",
            "4 Classe attendue:  0  Classification:  0\n",
            "5 Classe attendue:  1  Classification:  1\n",
            "6 Classe attendue:  0  Classification:  0\n",
            "7 Classe attendue:  1  Classification:  1\n",
            "8 Classe attendue:  1  Classification:  1\n",
            "9 Classe attendue:  0  Classification:  0\n",
            "10 Classe attendue:  0  Classification:  0\n",
            "11 Classe attendue:  0  Classification:  0\n",
            "12 Classe attendue:  0  Classification:  0\n",
            "13 Classe attendue:  0  Classification:  0\n",
            "14 Classe attendue:  0  Classification:  1\n",
            "-------------\n",
            "Précision sur les donnees de tests = 93.33333333333333%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooD5AEwKsxcf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZbvOnj5tkFa"
      },
      "source": [
        "PRECISION SUR LES DONNEES D'APPRENTISSAGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVXqbuphsJ8Z",
        "outputId": "a9bf55bc-d26c-41b6-9bb1-858a6c72223f"
      },
      "source": [
        "nb_classifications = 0;\r\n",
        "nb_bonnes_classifications = 0\r\n",
        "for i in range(0,train_x.shape[0]):\r\n",
        "\r\n",
        "    # On récupere les informations\r\n",
        "    donneesSonar = train_x[i].reshape(1, 60)\r\n",
        "    classificationAttendue = train_y[i].reshape(1, 2)\r\n",
        "\r\n",
        "    # On réalise la classification\r\n",
        "    prediction_run = session.run(classifications, feed_dict={tf_neurones_entrees_X: donneesSonar})\r\n",
        "\r\n",
        "    # On calcule la précision de la classification à l'aide de la formule établie auparavant\r\n",
        "    accuracy_run = session.run(formule_precision, feed_dict={tf_neurones_entrees_X: donneesSonar, tf_valeurs_reelles_Y: classificationAttendue})\r\n",
        "\r\n",
        "    nb_classifications = nb_classifications + 1\r\n",
        "    if (accuracy_run * 100 == 100):\r\n",
        "        nb_bonnes_classifications = nb_bonnes_classifications + 1\r\n",
        "\r\n",
        "\r\n",
        "print(\"Précision sur les donnees d'apprentissage = \" + str((nb_bonnes_classifications / nb_classifications) * 100) + \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Précision sur les donnees d'apprentissage = 88.02083333333334%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXwy0WYYtqCL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJTfhaJTtuhP"
      },
      "source": [
        "PRECISION SUR L'ENSEMBLE DES DONNEES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEb6ov_jtqfj",
        "outputId": "dd4a5f12-7cc3-49ed-84f3-f29ed99a13ba"
      },
      "source": [
        "nb_classifications = 0;\r\n",
        "nb_bonnes_classifications = 0\r\n",
        "for i in range(0,207):\r\n",
        "\r\n",
        "    prediction_run = session.run(classifications, feed_dict={tf_neurones_entrees_X:X[i].reshape(1,60)})\r\n",
        "    accuracy_run = session.run(formule_precision, feed_dict={tf_neurones_entrees_X:X[i].reshape(1,60), tf_valeurs_reelles_Y:Y[i].reshape(1,2)})\r\n",
        "\r\n",
        "    nb_classifications = nb_classifications + 1\r\n",
        "    if (accuracy_run * 100 == 100):\r\n",
        "        nb_bonnes_classifications = nb_bonnes_classifications + 1\r\n",
        "\r\n",
        "\r\n",
        "print(\"Précision sur l'ensemble des données = \" + str((nb_bonnes_classifications / nb_classifications) * 100) + \"%\")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "session.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Précision sur l'ensemble des données = 88.40579710144928%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}